---
title: "Explore habitat suitability modle workflow"
author: "Caitlin Mothes"
date: "`r Sys.Date()`"
---

A lot of this code and recommended workflow was derived from the `ENMeval` package vignette: <https://jamiemkass.github.io/ENMeval/articles/ENMeval-2.0-vignette.html>

# Setup

```{r}
source("setup.R")

# set up multisession for furrr
#plan(multisession)

# set some parameters for testing
species <- "Agelaius phoeniceus"
```

## Read in inputs

```{r}
# occurrences
occ <- read_csv("data/input_occ/final_model_occurrences.csv") %>% 
  # for testing keep just one species
  filter(species == species)

# AOI
gma <- read_sf("data/ft_collins_GMA.shp")

# create a couple test predictors
files <- c(
  "data/input_raw/Green_Space/nic_green_space_2015.shp",
  "data/input_raw/Street_Centerlines/Street_Centerlines.shp",
  "data/input_raw/Hydrology/Hydrology.shp"
)


preds <- purrr::map(files, ~process_preds(.x, template = gma, resolution = 500, distance = TRUE, save = FALSE))

preds_stack <- terra::rast(preds)

# convert occ to spatial for some downstream stuff
occ_sf <- occ %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>% 
  st_transform(crs = crs(preds_stack))
      
```

### Map out vars and points

```{r}
tmap_mode("view")

tm_shape(preds_stack) +
  tm_raster(style = "cont", legend.show = FALSE) +
  tm_facets(as.layers = TRUE) +
  tm_shape(occ_sf) +
  tm_dots(col = "species")
```

### Background Points

Sample 10,000 random background points

*In the future, consider target-group background points.*

```{r}
bg <- dismo::randomPoints(raster::raster(preds_stack), n = 10000) %>% as.data.frame() %>% 
  rename(longitude = x, latitude = y) %>% 
  st_as_sf(coords = c("longitude","latitude"), crs = crs(preds_stack))
```

Plot bg points

```{r}
qtm(preds_stack[[1]]) +
  qtm(bg)
```

### 

# Execute

Prepare inputs for model specifications:

```{r}
# convert points to a df of just lat/long for model input
occ_mod <- occ_sf %>%
  mutate(latitude = st_coordinates(.)[, "Y"], longitude = st_coordinates(.)[, "X"]) %>%
  dplyr::select(longitude, latitude) %>%
  st_drop_geometry()

bg_mod <- bg %>%
  mutate(latitude = st_coordinates(.)[, "Y"], longitude = st_coordinates(.)[, "X"]) %>%
  dplyr::select(longitude, latitude) %>%
  st_drop_geometry()
  
# convtert SpatRaster to RasterStack
preds_mod <- raster::brick(preds_stack)
```

## Run Maxent

Using the `maxnet` package which removes the needs to deal with rJava and maxent.jar files.

Notes:

-   Partitions uses 5 groups by default. To change this add `partition.settings = list(kfolds = ##)`

-   Uses variable clamping by default, meaning prediction extrapolations are restricted to the upper and lower bounds of predictor variables

-   Using `parallell = TRUE` this ran in seconds compared to minutes++

```{r}
all_mods <- ENMevaluate(occs = occ_mod, envs = preds_mod, 
                    #bg = bg_mod, #can also leave this out and set 'n.bg = ##' instead
                    algorithm = 'maxnet', partitions = 'randomkfold', 
                    taxon.name = species,
                    tune.args = list(fc = c("L","LQ","LQH","H"), rm = 1:5),
                    parallel = TRUE
                    )
```

## Visualize tuning results

Look at omission rates and validation AUC

```{r}
evalplot.stats(e = all_mods, stats = c("or.mtp", "auc.val"), color = "fc", x.var = "rm", 
               error.bars = FALSE)
```

# Model Selection

Choose top model based lowest omission rate and highest validation AUC (to break ties)

-   Idea, give the user the option to choose best model or most complex/simple model

```{r}
top_mod_args <- eval.results(all_mods) %>%
  filter(or.10p.avg == min(or.10p.avg)) %>%
  filter(auc.val.avg == max(auc.val.avg))

selected_mod <- eval.models(all_mods)[[top_mod_args$tune.args]]
```

Look at model coefficients and response curves

```{r}
# coefficients
tibble::enframe(selected_mod$betas)

# response curves
plot(selected_mod, type = "cloglog")
```

# Model Predictions

```{r}
predicted_mod <- eval.predictions(all_mods)[[top_mod_args$tune.args]]

tm_shape(predicted_mod) +
  tm_raster(style = "cont") +
  tm_shape(eval.bg(all_mods))
```

```{r}
dev.off()
plot(predicted_mod)

# visualize training sets (this really only matter with spatial blocks)
points(eval.bg(all_mods), pch = 3, col = eval.bg.grp(all_mods), cex = 0.5)
points(eval.occs(all_mods), pch = 21, bg = eval.occs.grp(all_mods))
```

Or interactive:

```{r}
tm_shape(predicted_mod) +
  tm_raster(style = "cont", palette = "plasma")
```

### Compare the most simple and complex models

```{r}
# Finally, let's cut the plotting area into two rows to visualize the predictions 
# side-by-side.
par(mfrow=c(2,1), mar=c(2,1,2,0))
# The simplest model: linear features only and high regularization.
plot(eval.predictions(all_mods)[['fc.L_rm.5']], #ylim = c(-30,20), xlim = c(-90,-30), 
     legend = FALSE, main = 'L_5 prediction')
# The most complex model: linear, quadratic, and hinge features with low regularization
plot(eval.predictions(all_mods)[['fc.LQH_rm.1']], #ylim = c(-30,20), xlim = c(-90,-30), 
     legend = FALSE, main = 'LQH_1 prediction')
```

# Null Models

Allow us to calculate significance and effect size of model metrics by comparing it to null models built with random data

Calculate null models using settings of best model

*Note, parallel = TRUE does not work on this for some reason, took 4 minutes without*

```{r}
null_mod <- ENMnulls(
  e = all_mods,
  mod.settings = list(fc = "LQ", rm = 5), no.iter = 100
)
```

Plot results and view p-values

```{r}
evalplot.nulls(null_mod, stats = c("or.10p", "auc.val"), plot.type = "histogram")
```

```{r}
null.emp.results(null_mod)
```

# Compile Metadata

```{r}
# Generate a rangeModelMetadata object based on the information stored in the 
# ENMevaluate object.
rmm <- eval.rmm(all_mods)
# We can fill in the model selection rules based on the sequential criteria we chose.
rmm$model$selectionRules <- "lowest 10 percentile omission rate, 
break ties with average validation AUC"
# We can also enter our optimal model settings and the details of our optimal 
# model's prediction.
rmm$model$finalModelSettings <- paste(top_mod_args$fc, top_mod_args$rm)
rmm$prediction$continuous$minVal <- raster::cellStats(predicted_mod, min)
rmm$prediction$continuous$maxVal <- raster::cellStats(predicted_mod, max)
rmm$prediction$continuous$units <- "suitability (cloglog transformation)"

```

# Next Steps

-   Figure out what to save from this rmm output, OR create our own output

-   Things to include in output:

    -   Model params/methods (stuff from rmm)

    -   Predicted raster

    -   response curves

    -   null model results table

    -   Maybe results for: best model, simple model and complex model?

    -   tuning results plot

```{r}
# will still want to remove a lot of this, maybe best to just specify what we DO want
cleaned_rmm <- map_dfr(rmm, function(x){
  enframe(x)}
  ) %>% remove_nulls()

cleaned_df <- tibble(name = names(cleaned_rmm), value = cleaned_rmm)
```
